\documentclass[conference, 10ppt]{IEEEtran}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{comment}
%
%\usepackage{slashbox}
\usepackage{url}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm,algpseudocode}
\algrenewcommand\algorithmicindent{0.9em}%
\usepackage{soul}
\usepackage{xspace}
\usepackage{subfigure}
\usepackage[group-separator={,}, group-minimum-digits=4]{siunitx}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{xcolor,colortbl}

%
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
 
\begin{document}
%
\newcommand{\todo}[1]{\color{red}\textbf{\hl{#1}}\color{black}\xspace}
%\newcommand{\todo}[1]{}
\newcommand{\rom}[1]{\expandafter{\romannumeral #1\relax}}
%
\title{Performance Model of Iterated SpMV for Distributed System.}

\author{\IEEEauthorblockN{Md Maruf Hossain}
\IEEEauthorblockA{Dept. Computer Science \\
\textit{University of North Carolina at Charlotte}\\
Charlotte, USA \\
mhossa10@uncc.edu}
\and
\IEEEauthorblockN{Erik Saule}
\IEEEauthorblockA{Dept. Computer Science \\
\textit{University of North Carolina at Charlotte}\\
Charlotte, USA \\
esaule@uncc.edu}
}
%
%%
%%
\maketitle
%%

\begin{abstract}
Many applications rely on basic sparse linear algebra operations from numerical solvers to graph analysis 
algorithms. Yet, the performance of these operations is still reasonably unknown. Users and practitioners 
rely on the rule of thumb understanding of what typically works best for some application domain.
\\ 
This paper aims at providing an overall framework for the distributed system to think about the performance 
of sparse applications. We use the sparse matrix-vector(SpMV) multiplication as the representative of the 
experiments. We model the performance of multiple SpMV implementations on distributed systems. 
 We model the performance of different modes of execution of SpMV using linear and polynomial regression models for a distributed system. 
 The models enable us to predict how to partition and represent the sparse matrix to optimize the performance of iterated SpMV on a cluster with 225 cores.
\end{abstract}
%
%%
%%

\begin{IEEEkeywords}
SpMV, MPI, Graph Partitioning
\end{IEEEkeywords}

\section{Introduction}
%%%%%%%%% Why %%%%%%%%%
Sparse matrix-vector multiplication(\textit{SpMV}) is one of the
fundamental operations in sparse linear algebra. It is critical to
solving linear systems and is widely used in engineering and scientific
applications~\cite{gleich2015pagerank, saad2003iterative, dytrych2016efficacy}. 
Distributed memory systems have entered a new age with
the popularization of departmental clusters to support these
scientific and engineering applications.  Many different approaches
have been proposed to improve \textit{SpMV} performance on
clusters. But choosing the right approach still mostly relies on the rule
of thumbs. We posit that building models to predict the performance of
different configurations is the only way to make better-informed
decisions.

In this paper, we propose a linear and polynomial \textit{Support
  Vector Regression}(\textit{SVR})~\cite{awad2015support} model to predict and analyze the
run time of \textit{SpMV} on the distributed system for different ways
to execute the operation. Our system will analyze the predicted run
time and return the best possible algorithm for the system. The
performance of the \textit{SpMV} mainly depends on the size and
structure of the matrices and the architecture of the system. 

%%%%%%%%% How %%%%%%%%
To perform sparse matrix-vector multiplication(\textit{SpMV}) on
distributed systems, the most common mechanism is to partition the
matrix into multiple parts and perform \textit{SpMV} on each part
individually in the different processors. Good partitioning can ensure
better load balance and limits the volume of communication between the
underlying \textit{MPI process}. Many partitioning
algorithms have been proposed to ensure good load balance and to
minimize \textit{MPI communications}~\cite{deveci2015hypergraph, karypis1995multilevel,
  kaya2013analysis}.

%%%%%%%%% What %%%%%%%%
In this paper, we explore two partitioning modes (Uniform
2D-Partitioning and 1D Row Partitioning) and the performance of the
different \textit{SpMV} representation based on these partitioning
mechanisms on distributed systems. We develop a linear and a
polynomial support vector regression (\textit{SVR}) performance models
for SpMV operations on the distributed system for these different
techniques. The models are accurate enough to predict the best
configuration to execute SpMV given a matrix.

\section{Related Work}
Lots of different performance model for the \textit{SpMV}  exist. In particular, Guo and Wang \textit{et.al.}~\cite{guo2013performance} presented a 
performance model for the general purpose GPU. They provide a linear model that can predict the performance of the matrix based on the 
matrix strides size and nonzero per row. Guo \textit{et.at.}~\cite{guo2018performance} also provide a \textit{SVM} machine learning 
model for the GPU. 

\section{SpMV on Distributed Systems}
In \textit{2D-Uniform partitioning},  matrices
are partitioned into both row-wise and column-wise.  There is a choice
in the number of column partitions and row partitions. However, for
iterated SpMV applications, it is common practice to pick a symmetric
partitioning scheme and a number of processors that is a perfect
square. Each MPI process handles one of the portions of the matrix.
Load imbalance is a common issue in 2D partitioning. Many matrices
tend to have a band structure that tends to build very imbalanced
partitioning.  2D-Uniform partitioning can balance the number of rows
and columns but can have a significant load imbalance in the number
of non-zeros in the blocks.

Boman and Devine et al.~\cite{boman2013scalable} noted that
randomizing the order of the matrix can provide good load balance for
SpMV in 2D-Uniform partitioning and is used in scientific
applications~\cite{dytrych2016efficacy}. We adopt this strategy for
our 2D uniform partitioning technique. Each row (and corresponding
vector entry) is assigned to a random process.  Since the expected
number of rows and non-zeros is uniform for all processes, this method
generally achieves a good load balance.

If $P=p^2$ is the number of processes
and $matrix\_size$ is the size of the matrix then each processor
should contain the same
number$\Bigl\lceil\dfrac{matrix\_size}{p}\Bigr\rceil$ of rows and
columns. The last block can be padded with zeros to make blocks of the name size.

With 2D-Uniform partitioning, the execution of the iterated SpMV is
done using the classic methods. Each process performs its local
multiplication. The values are then reduced within each row onto the
diagonal process. These reduced values are then broadcast along with the
column processes for the next iteration of the calculation.

In \textit{1D-Row Partitioning}, matrices are split into row-wise
only. Balancing the load between the processors and minimizing the
communication boils down to solving a K-way graph partitioning
problem~\cite{kaya2013analysis}.  We can define the ID-Row(K-way)
partitioning for a graph $G=(V,E)$ with $|V|=n$, partition $V$ into
$k$ subsets, $V_1, V_2, \dots, V_k$ such that $V_i \cap V_j=\phi$ for
$i\neq j$, $|V_i| = n/k$, and $\cup_i V_i = V$, and the number of
edges of $E$ whose incident vertices belong to different subsets is
minimized. Each partition will allocate to a different MPI
process. We base the 1D row partitioning technique that we will model
on this strategy and we choose the METIS graph partitioning
tool~\cite{karypis1995multilevel} to provide the precise row
partitioning.

In 1D-Row partitioning, one process contains small portions of rows
and their corresponding columns. To perform SpMV on the 1D-Row
partitioning, a processor can hold either the portion of the \textbf{vector
  elements} corresponding to their row elements or the full vector.
We call the strategy where the process holds the entire vector \textit{Global
  1D-Row SpMV}(G1DR-SpMV). In this strategy, all MPI processes
perform matrix multiplication locally on the part of the matrix that
belongs to them. After matrix multiplication, an ALL$\_$Gatherv takes
place to share the updated value of the vector.

We call the strategy where a process only holds the portion of the
vector that it needs \textit{Local 1D-Row SpMV}(L1DR-SpMV).  In this
algorithm, initially, a process performs local matrix multiplication
on the non zero elements whose column belong to the local vector.  For
the rest of the non zero elements whose vector elements belong to the
other processes, it retrieves the relevant part of the vector using
communication tailored to each process using standard MPI point to
point communication primitives.

The METIS K-way partitioning technique minimizes the communications
performed by the Local 1D-Row SpMV. Therefore if METIS can partition
the graph well, one would expect the Local 1D-Row to perform very few
communications. However, if the partitioning is not good, the custom
message strategy only saves few communications and the Global 1D-Row
SpMV would benefit from the effectiveness of MPI Collectives. There is
no point in designing a local 2D scheme since the randomized
partitioning ensures there is little saving to be expected from
customized communications.

The local matrix stored by the nodes can be represented in a number of
formats. We picked the two most popular formats: CSR and COO.


\section{\todo{SpMV on AVX-512 Architecture (Skylake)}}
SpMV on the distributed system initially can be divided into two parts, 
\begin{itemize}
\item MPI Communication: latency to share the vector and gather the results from the MPI process. 
\item Core SpMV Calculation: latency to calculate part of SpMV for a single MPI process.
\end{itemize}

\subsection{MPI communication}
\begin{figure}[hbt!]
	\centering
	\subfigure[MPI All\_GatherV.]{\includegraphics[width=0.48\linewidth]{figures/osu/skylake_mpi_allgatherv.pdf}		\label{fig:osu-skylake-allgatherv}}
	\subfigure[MPI All\_Reduce.]{\includegraphics[width=0.48\linewidth]{figures/osu/skylake_mpi_allreduce.pdf}		\label{fig:osu-skylake-reduce}}
	\caption{Latency of the MPI collectives(All\_GatherV and All\_Reduce) on Skylake.}
	\label{fig:osu-collectives}
\end{figure}


\subsection{Core SpMV Calculation}
We can represent the basic SpMV by $y=y+Val*x$, which contains two floating point operation(multiply and addition). 
So, we can say we need to calculate NNZ(number of non zeros) times FMA(fused multiply addition) to perform SpMV.
In our experiment we divide the core SpMV calculation into two major parts,
\begin{itemize}
\item Run time for FMA: $L_{FMA}\times NNZ$, where $L_{FMA}$ is the latency of a single FMA and $NNZ$ is number of nonzeros.
\item Read-write latency: $L_{RW}$
\end{itemize} 

\subsubsection{Memory Access bandwidth}
\begin{figure}[hbt!]
	\centering
	\subfigure[Single Precision.]{\includegraphics[width=0.48\linewidth]{figures/stream/skylake_mpi_copy_single_precision.pdf}		\label{fig:stream-single-copy}}
	\subfigure[Double Precision.]{\includegraphics[width=0.48\linewidth]{figures/stream/skylake_mpi_copy_double_precision.pdf}		\label{fig:stream-double-copy}}
	\caption{Single and double precision memory access bandwidth on Skylake processor.}
	\label{fig:stream-copy-bandwidth}
\end{figure}

\begin{figure}[hbt!]
	\centering
	\subfigure[Double Precision.]{\includegraphics[width=0.48\linewidth]{figures/stream/skylake_mpi_copy_single_precision_time.pdf}		\label{fig:stream-double-copy}}
	\subfigure[Single Precision.]{\includegraphics[width=0.48\linewidth]{figures/stream/skylake_mpi_copy_double_precision_time.pdf}		\label{fig:stream-single-copy}}
	\caption{single and double precision memory access time on Skylake processor.}
	\label{fig:stream-copy-bandwidth}
\end{figure}

Figure~\ref{fig:stream-copy-bandwidth} shows, when the size of the data is more than 1GB the bandwidth shows the consistency 
with the size of the data(132 GB/s).


\subsubsection{Roofline Model for FMA}
The throughput of Skylake and Cascade Lake are same as 2 instructions per cycle and the latency of FMA is 4 cycles for both of them. So there is a potential of pipelinling($2\times 4 = 8$) to get the optimal results. Now, both of the architecture 
has 512-bit register that can give the ability of the vectorization. For 64-bits floating point operation it can give vector 
width 8 and for 32-bits it can give at max vector width 16. To find the peak performance of the FMA and to avoid the 
read-write latency we need to setup the benchmark that datasets can be contained in the register. Now, both of the 
architecture have 32 registers. We can populate the pipeline by using sufficient ampunt of work. By varying the 
number of fused-multiply-addition calculation, we can find out the performance limitation. From the informations of the 
processors, we can say that 4 cycles required for FMA and the throughput of the FMA is 2 instruction per cycle, that means 
we should at least use $4\times 2=8$ instruction at a time to populate the pipeline.

\begin{figure}[hbt!]
	\centering
	\subfigure[Vector Width: 4]{\includegraphics[width=0.48\linewidth]{figures/fma/skylake_mpi_fma_roofline_model_for_vec_4.pdf}		\label{fig:mpi-skl-vec-4}}
	\subfigure[Vector Width: 8]{\includegraphics[width=0.48\linewidth]{figures/fma/skylake_mpi_fma_roofline_model_for_vec_8.pdf}		\label{fig:mpi-skl-vec-8}}
	\caption{Skylake: (MPI)Roofline model for bandwidth for FMA operation}
	\label{fig:mpi-skl-roofline}
\end{figure}

\begin{comment}
\begin{figure}[hbt!]
	\centering
	\subfigure[Vector Width: 4]{\includegraphics[width=0.48\linewidth]{figures/fma/cascade_lake_mpi_fma_roofline_model_for_vec_4.pdf}		\label{fig:mpi-cas-vec-4}}
	\subfigure[Vector Width: 8]{\includegraphics[width=0.48\linewidth]{figures/fma/cascade_lake_mpi_fma_roofline_model_for_vec_8.pdf}		\label{fig:mpi-cas-vec-8}}
	\caption{Cascade Lake: (MPI)Roofline model for bandwidth for FMA operation}
	\label{fig:mpi-cas-roofline}
\end{figure}
\end{comment}

\begin{figure}[hbt!]
	\centering
	\subfigure[Vector Width: 4]{\includegraphics[width=0.48\linewidth]{figures/fma/skylake_omp_fma_roofline_model_for_vec_4.pdf}		\label{fig:omp-skl-vec-4}}
	\subfigure[Vector Width: 8]{\includegraphics[width=0.48\linewidth]{figures/fma/skylake_omp_fma_roofline_model_for_vec_8.pdf}		\label{fig:omp-skl-vec-8}}
	\caption{Skylake: (Shared memory parallel)Roofline model for bandwidth for FMA operation}
	\label{fig:omp-skl-roofline}
\end{figure}

\begin{comment}
\begin{figure}[hbt!]
	\centering
	\subfigure[Vector Width: 4]{\includegraphics[width=0.48\linewidth]{figures/fma/cascade_lake_omp_fma_roofline_model_for_vec_4.pdf}		\label{fig:omp-cas-vec-4}}
	\subfigure[Vector Width: 8]{\includegraphics[width=0.48\linewidth]{figures/fma/cascade_lake_omp_fma_roofline_model_for_vec_8.pdf}		\label{fig:omp-cas-vec-8}}
	\caption{Cascade lake: (Shared memory parallel)Roofline model for bandwidth for FMA operation}
	\label{fig:omp-cas-roofline}
\end{figure}
\end{comment}

We can estimate the theoritical peak performance a single FMA by the following equation,
\begin{eqnarray}
P\ =\ Base\_Clock\_Frequency\times Vector\_Width\times \frac{FLOPs}{Instruction}.
\end{eqnarray}

\section{SpMV Model from Micro-Benchmark}
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=0.96\linewidth]{figures/spmv_hardware_model.pdf}
	\caption{Structure of the SpMV model from micro-benchmark.}
	\label{fig:spmv-model-from-benchmark}
\end{figure}
\subsection{Random 2D-SpMV}
\subsubsection{CSR}
\begin{table}[htb]
\caption{SpMV Property.}
\label{tab:csr-spmv-property}
\centering
\begin{tabular}[c]{| l | c | c | c | c |}
\hline
\multirow{2}{*}{Array} & \multirow{2}{*}{Elements Size} & \multirow{2}{*}{Data Type} & \multicolumn{2}{ | c |}{Access Type} \\ \cline{4-5}
  &  &  & CSR & COO \\ \hline
rowA & 2$\times$ppn$\times$rpp & Integer & Sequential & Sequential \\ \hline
colA & ppn$\times$nnz & Integer & Sequential & Sequential  \\ \hline
valA & ppn$\times$nnz & Floating & Sequential &  Sequential \\ \hline
x & ppn$\times$nnz & Floating & Random &  Random \\ \hline
y & ppn$\times$rpp & Floating & Sequential & Random  \\ \hline
\end{tabular}
\end{table}

\begin{comment}
\begin{table}[htb]
\caption{SpMV(only Matrix-Multiplication) on Random 2D Partitioning(Single Precision on Skylake).}
\label{tab:spmv-matmul-2d-single}
\centering
\begin{tabular}[c]{| l | c | c | c | c | c |}
\hline
Matrices & Nodes & nProcesses & Actual Time & Predicted Time & Error\% \\ \hline
\multirow{4}{*}{333SP}  &  4  &  144  &  3.20E-03  &  3.76E-03  &  17.5  \\ \cline{2-6}
  &  5  &  169  &  2.75E-03  &  3.10E-03  &  12.7  \\ \cline{2-6}
  &  7  &  225  &  1.89E-03  &  2.20E-03  &  16.1  \\ \cline{2-6}
  &  8  &  256  &  1.69E-03  &  1.88E-03  &  11.4  \\ \hline
\multirow{4}{*}{AS365}  &  4  &  144  &  3.29E-03  &  3.91E-03  &  18.9  \\ \cline{2-6}
  &  5  &  169  &  2.83E-03  &  3.23E-03  &  14.0  \\ \cline{2-6}
  &  7  &  225  &  1.95E-03  &  2.29E-03  &  17.6  \\ \cline{2-6}
  &  8  &  256  &  1.74E-03  &  1.97E-03  &  13.1  \\ \hline
\multirow{4}{*}{M6}  &  4  &  144  &  2.98E-03  &  3.40E-03  &  14.2  \\ \cline{2-6}
  &  5  &  169  &  2.48E-03  &  2.80E-03  &  12.9  \\ \cline{2-6}
  &  7  &  225  &  1.76E-03  &  1.97E-03  &  11.9  \\ \cline{2-6}
  &  8  &  256  &  1.57E-03  &  1.69E-03  &  7.2  \\ \hline
\multirow{4}{*}{NLR}  &  4  &  144  &  3.67E-03  &  4.54E-03  &  23.6  \\ \cline{2-6}
  &  5  &  169  &  3.18E-03  &  3.77E-03  &  18.5  \\ \cline{2-6}
  &  7  &  225  &  2.42E-03  &  2.70E-03  &  11.8  \\ \cline{2-6}
  &  8  &  256  &  1.96E-03  &  2.32E-03  &  18.5  \\ \hline
\multirow{4}{*}{hugetrace-00010}  &  4  &  144  &  9.27E-03  &  1.09E-02  &  17.1  \\ \cline{2-6}
  &  5  &  169  &  9.07E-03  &  9.39E-03  &  3.5  \\ \cline{2-6}
  &  7  &  225  &  7.10E-03  &  7.26E-03  &  2.2  \\ \cline{2-6}
  &  8  &  256  &  5.63E-03  &  6.47E-03  &  15.0  \\ \hline
\multirow{4}{*}{road\_central}  &  4  &  144  &  9.51E-03  &  1.11E-02  &  16.8  \\ \cline{2-6}
  &  5  &  169  &  9.64E-03  &  9.66E-03  &  0.2  \\ \cline{2-6}
  &  7  &  225  &  7.70E-03  &  7.55E-03  &  2.0  \\ \cline{2-6}
  &  8  &  256  &  6.04E-03  &  6.77E-03  &  12.0  \\ \hline
\multirow{4}{*}{road\_usa}  &  4  &  144  &  1.66E-02  &  1.97E-02  &  18.8  \\ \cline{2-6}
  &  5  &  169  &  1.67E-02  &  1.72E-02  &  3.1  \\ \cline{2-6}
  &  7  &  225  &  1.30E-02  &  1.35E-02  &  3.3  \\ \cline{2-6}
  &  8  &  256  &  1.04E-02  &  1.21E-02  &  15.7  \\ \hline
\end{tabular}
\end{table}
\end{comment}

\begin{table}[htb]
\caption{Overall SpMV on Random CSR 2D Partitioning(on Skylake).}
\label{tab:overall-spmv-csr-2d-single}
\centering
\begin{tabular}[c]{| l | c | c | c | c | c |}
\hline
\multirow{2}{*}{Matrices} & \multirow{2}{*}{Nodes} & \multirow{2}{*}{Processes} & \multicolumn{2}{| c |}{Time(s)} & \multirow{2}{*}{Error\%} \\ \cline{4-5}
  &  &  & Actual & Predicted &  \\ \hline
\multirow{4}{*}{333SP}   &  4  &  144  &  7.2E-03  &  7.5E-03  &  4.1\% \\ \cline{2-6}
  &  5  &  169  &  5.7E-03  &  6.6E-03  &  15.1\% \\ \cline{2-6}
  &  7  &  225  &  4.7E-03  &  5.2E-03  &  8.8\% \\ \cline{2-6}
  &  8  &  256  &  4.2E-03  &  4.5E-03  &  6.1\% \\ \hline
\multirow{4}{*}{AS365}  &  4  &  144  &  7.4E-03  &  7.8E-03  &  4.7\% \\ \cline{2-6}
  &  5  &  169  &  5.8E-03  &  6.8E-03  &  15.8\% \\ \cline{2-6}
  &  7  &  225  &  4.7E-03  &  5.3E-03  &  13.5\% \\ \cline{2-6}
  &  8  &  256  &  4.4E-03  &  4.6E-03  &  6.1\% \\ \hline
\multirow{4}{*}{M6}   &  4  &  144  &  6.8E-03  &  7.0E-03  &  2.7\% \\ \cline{2-6}
  &  5  &  169  &  5.3E-03  &  6.1E-03  &  15.0\% \\ \cline{2-6}
  &  7  &  225  &  4.4E-03  &  4.8E-03  &  9.4\% \\ \cline{2-6}
  &  8  &  256  &  4.0E-03  &  4.2E-03  &  4.7\% \\ \hline
\multirow{4}{*}{NLR}   &  4  &  144  &  8.2E-03  &  8.7E-03  &  6.3\% \\ \cline{2-6}
  &  5  &  169  &  6.5E-03  &  7.6E-03  &  17.8\% \\ \cline{2-6}
  &  7  &  225  &  5.4E-03  &  6.0E-03  &  10.9\% \\ \cline{2-6}
  &  8  &  256  &  4.8E-03  &  5.2E-03  &  9.0\% \\ \hline
\multirow{4}{*}{hugetrace-00010}  &  4  &  144  &  2.3E-02  &  2.2E-02  &  2.9\% \\ \cline{2-6}
  &  5  &  169  &  2.0E-02  &  2.0E-02  &  1.6\% \\ \cline{2-6}
  &  7  &  225  &  1.7E-02  &  1.6E-02  &  5.6\% \\ \cline{2-6}
  &  8  &  256  &  1.4E-02  &  1.5E-02  &  8.6\% \\ \hline
\multirow{4}{*}{road\_central}  &  4  &  144  &  2.5E-02  &  2.4E-02  &  2.5\% \\ \cline{2-6}
  &  5  &  169  &  2.2E-02  &  2.2E-02  &  1.4\% \\ \cline{2-6}
  &  7  &  225  &  1.9E-02  &  1.8E-02  &  5.2\% \\ \cline{2-6}
  &  8  &  256  &  1.5E-02  &  1.6E-02  &  7.2\% \\ \hline
\multirow{4}{*}{road\_usa}  &  4  &  144  &  3.6E-02  &  4.1E-02  &  15.0\% \\ \cline{2-6}
  &  5  &  169  &  3.6E-02  &  3.8E-02  &  5.2\% \\ \cline{2-6}
  &  7  &  225  &  3.1E-02  &  3.1E-02  &  1.6\% \\ \cline{2-6}
\  &  8  &  256  &  2.7E-02  &  2.8E-02  &  6.6\% \\ \hline
\end{tabular}
\end{table}

\subsubsection{COO}
\begin{table}[htb]
\caption{Overall SpMV on Random COO 2D Partitioning(on Skylake).}
\label{tab:overall-spmv-coo-2d-single}
\centering
\begin{tabular}[c]{| l | c | c | c | c | c |}
\hline
\multirow{2}{*}{Matrices} & \multirow{2}{*}{Nodes} & \multirow{2}{*}{Processes} & \multicolumn{2}{| c |}{Time(s)} & \multirow{2}{*}{Error\%} \\ \cline{4-5}
  &  &  & Actual & Predicted &  \\ \hline
\multirow{4}{*}{333SP}   &  4  &  144  &  5.4E-03  &  5.6E-03  &  3.7\% \\ \cline{2-6}
  &  5  &  169  &  4.2E-03  &  5.0E-03  &  19.6\% \\ \cline{2-6}
  &  7  &  225  &  3.5E-03  &  4.0E-03  &  15.5\% \\ \cline{2-6}
  &  8  &  256  &  3.4E-03  &  3.5E-03  &  4.3\% \\ \hline
\multirow{4}{*}{AS365}  &  4  &  144  &  5.5E-03  &  5.8E-03  &  4.0\% \\ \cline{2-6}
  &  5  &  169  &  4.3E-03  &  5.1E-03  &  19.7\% \\ \cline{2-6}
  &  7  &  225  &  3.6E-03  &  4.1E-03  &  13.6\% \\ \cline{2-6}
  &  8  &  256  &  3.6E-03  &  3.6E-03  &  0.1\% \\ \hline
\multirow{4}{*}{M6}  &  4  &  144  &  5.0E-03  &  5.2E-03  &  3.5\% \\ \cline{2-6}
  &  5  &  169  &  3.9E-03  &  4.6E-03  &  18.7\% \\ \cline{2-6}
  &  7  &  225  &  3.4E-03  &  3.8E-03  &  9.7\% \\ \cline{2-6}
  &  8  &  256  &  3.3E-03  &  3.3E-03  &  0.8\% \\ \hline
\multirow{4}{*}{NLR}  &  4  &  144  &  6.1E-03  &  6.4E-03  &  4.9\% \\ \cline{2-6}
  &  5  &  169  &  4.8E-03  &  5.7E-03  &  19.6\% \\ \cline{2-6}
  &  7  &  225  &  4.0E-03  &  4.6E-03  &  15.2\% \\ \cline{2-6}
  &  8  &  256  &  3.9E-03  &  4.0E-03  &  2.8\% \\ \hline
\multirow{4}{*}{hugetrace-00010}   &  4  &  144  &  1.7E-02  &  1.6E-02  &  4.2\% \\ \cline{2-6}
  &  5  &  169  &  1.3E-02  &  1.5E-02  &  13.7\% \\ \cline{2-6}
  &  7  &  225  &  1.1E-02  &  1.2E-02  &  9.5\% \\ \cline{2-6}
  &  8  &  256  &  1.0E-02  &  1.1E-02  &  9.2\% \\ \hline
\multirow{4}{*}{road\_central}  &  4  &  144  &  1.9E-02  &  1.8E-02  &  5.6\% \\ \cline{2-6}
  &  5  &  169  &  1.5E-02  &  1.7E-02  &  12.4\% \\ \cline{2-6}
  &  7  &  225  &  1.3E-02  &  1.4E-02  &  8.7\% \\ \cline{2-6}
\  &  8  &  256  &  1.2E-02  &  1.3E-02  &  8.1\% \\ \hline
\multirow{4}{*}{road\_usa}  &  4  &  144  &  2.9E-02  &  3.0E-02  &  3.7\% \\ \cline{2-6}
  &  5  &  169  &  3.2E-02  &  2.8E-02  &  12.6\% \\ \cline{2-6}
  &  7  &  225  &  2.3E-02  &  2.4E-02  &  5.4\% \\ \cline{2-6}
  &  8  &  256  &  2.0E-02  &  2.2E-02  &  7.6\% \\ \hline
\end{tabular}
\end{table}

\subsection{Local 1D-SpMV}
\subsubsection{CSR}
\begin{table}[htb]
\caption{Overall SpMV on Local CSR 1D-Row Partitioning(on Skylake).}
\label{tab:overall-spmv-csr-lk-single}
\centering
\begin{tabular}[c]{| l | c | c | c | c | c |}
\hline
\multirow{2}{*}{Matrices} & \multirow{2}{*}{Nodes} & \multirow{2}{*}{Processes} & \multicolumn{2}{| c |}{Time(s)} & \multirow{2}{*}{Error\%} \\ \cline{4-5}
  &  &  & Actual & Predicted &  \\ \hline
\multirow{4}{*}{333SP}   &  4  &  144  &  1.2E-03  &  1.3E-03  &  9.8\% \\ \cline{2-6}
  &  5  &  169  &  1.1E-03  &  1.2E-03  &  12.6\% \\ \cline{2-6}
  &  7  &  225  &  8.4E-04  &  9.7E-04  &  15.6\% \\ \cline{2-6}
  &  8  &  256  &  8.2E-04  &  8.8E-04  &  6.6\% \\ \hline
\multirow{4}{*}{AS365}  &  4  &  144  &  1.3E-03  &  1.5E-03  &  9.1\% \\ \cline{2-6}
  &  5  &  169  &  1.2E-03  &  1.3E-03  &  10.9\% \\ \cline{2-6}
  &  7  &  225  &  9.3E-04  &  1.0E-03  &  11.5\% \\ \cline{2-6}
  &  8  &  256  &  8.6E-04  &  9.3E-04  &  7.9\% \\ \hline
\multirow{4}{*}{M6}  &  4  &  144  &  1.2E-03  &  1.4E-03  &  12.5\% \\ \cline{2-6}
  &  5  &  169  &  1.1E-03  &  1.2E-03  &  15.4\% \\ \cline{2-6}
  &  7  &  225  &  8.9E-04  &  9.8E-04  &  10.7\% \\ \cline{2-6}
  &  8  &  256  &  8.1E-04  &  8.9E-04  &  9.3\% \\ \hline
\multirow{4}{*}{NLR}  &  4  &  144  &  1.4E-03  &  1.6E-03  &  9.3\% \\ \cline{2-6}
  &  5  &  169  &  1.3E-03  &  1.4E-03  &  8.9\% \\ \cline{2-6}
  &  7  &  225  &  9.8E-04  &  1.1E-03  &  12.0\% \\ \cline{2-6}
  &  8  &  256  &  9.6E-04  &  9.9E-04  &  3.0\% \\ \hline
\multirow{4}{*}{hugetrace-00010}   &  4  &  144  &  2.5E-03  &  2.7E-03  &  5.3\% \\ \cline{2-6}
 &  5  &  169  &  2.2E-03  &  2.4E-03  &  8.8\% \\ \cline{2-6}
 &  7  &  225  &  1.8E-03  &  1.9E-03  &  6.8\% \\ \cline{2-6}
  &  8  &  256  &  1.7E-03  &  1.7E-03  &  2.6\% \\ \hline
\multirow{4}{*}{road\_central}  &  4  &  144  &  2.7E-03  &  2.6E-03  &  2.1\% \\ \cline{2-6}
  &  5  &  169  &  2.4E-03  &  2.3E-03  &  1.5\% \\ \cline{2-6}
  &  7  &  225  &  1.9E-03  &  1.9E-03  &  0.5\% \\ \cline{2-6}
  &  8  &  256  &  1.8E-03  &  1.7E-03  &  1.2\% \\ \hline
\multirow{4}{*}{road\_usa}  &  4  &  144  &  4.1E-03  &  3.8E-03  &  7.9\% \\ \cline{2-6}
  &  5  &  169  &  3.4E-03  &  3.4E-03  &  0.7\% \\ \cline{2-6}
  &  7  &  225  &  2.8E-03  &  2.8E-03  &  1.4\% \\ \cline{2-6}
  &  8  &  256  &  2.6E-03  &  2.6E-03  &  0.7\% \\ \hline
\end{tabular}
\end{table}


\subsubsection{COO}
\begin{table}[htb]
\caption{Overall SpMV on Local COO 1D-Row Partitioning(on Skylake).}
\label{tab:overall-spmv-coo-lk-single}
\centering
\begin{tabular}[c]{| l | c | c | c | c | c |}
\hline
\multirow{2}{*}{Matrices} & \multirow{2}{*}{Nodes} & \multirow{2}{*}{Processes} & \multicolumn{2}{| c |}{Time(s)} & \multirow{2}{*}{Error\%} \\ \cline{4-5}
  &  &  & Actual & Predicted &  \\ \hline
\multirow{4}{*}{333SP}  &  4  &  144  &  1.2E-03  &  1.1E-03  &  8.6\% \\ \cline{2-6}
 &  5  &  169  &  1.1E-03  &  9.6E-04  &  11.8\% \\ \cline{2-6}
 &  7  &  225  &  8.9E-04  &  8.2E-04  &  8.7\% \\ \cline{2-6}
 &  8  &  256  &  8.2E-04  &  7.8E-04  &  5.6\% \\ \hline
\multirow{4}{*}{AS365}  &  4  &  144  &  1.4E-03  &  1.2E-03  &  11.0\% \\ \cline{2-6}
 &  5  &  169  &  1.2E-03  &  1.1E-03  &  7.0\% \\ \cline{2-6}
 &  7  &  225  &  9.3E-04  &  9.1E-04  &  2.4\% \\ \cline{2-6}
 &  8  &  256  &  8.5E-04  &  8.7E-04  &  2.9\% \\ \hline
\multirow{4}{*}{M6}  &  4  &  144  &  1.3E-03  &  1.2E-03  &  6.9\% \\ \cline{2-6}
 &  5  &  169  &  1.1E-03  &  1.0E-03  &  8.9\% \\ \cline{2-6}
 &  7  &  225  &  9.1E-04  &  8.7E-04  &  4.5\% \\ \cline{2-6}
 &  8  &  256  &  8.2E-04  &  8.5E-04  &  2.7\% \\ \hline
\multirow{4}{*}{NLR}  &  4  &  144  &  1.4E-03  &  1.3E-03  &  7.8\% \\ \cline{2-6}
 &  5  &  169  &  1.3E-03  &  1.1E-03  &  12.1\% \\ \cline{2-6}
 &  7  &  225  &  1.0E-03  &  9.6E-04  &  5.7\% \\ \cline{2-6}
 &  8  &  256  &  9.6E-04  &  9.2E-04  &  4.4\% \\ \cline{2-6}
\multirow{4}{*}{hugetrace-00010}  &  4  &  144  &  2.5E-03  &  2.2E-03  &  12.3\% \\ \cline{2-6}
 &  5  &  169  &  2.2E-03  &  2.0E-03  &  8.4\% \\ \cline{2-6}
 &  7  &  225  &  1.8E-03  &  1.6E-03  &  6.3\% \\ \cline{2-6}
 &  8  &  256  &  1.6E-03  &  1.5E-03  &  5.1\% \\ \hline
\multirow{4}{*}{road\_central}  &  4  &  144  &  2.5E-03  &  2.1E-03  &  13.0\% \\ \cline{2-6}
 &  5  &  169  &  2.1E-03  &  1.9E-03  &  9.5\% \\ \cline{2-6}
 &  7  &  225  &  1.7E-03  &  1.6E-03  &  7.2\% \\ \cline{2-6}
 &  8  &  256  &  1.6E-03  &  1.5E-03  &  9.2\% \\ \hline
\multirow{4}{*}{road\_usa}  &  4  &  144  &  3.8E-03  &  2.9E-03  &  22.9\% \\ \cline{2-6}
 &  5  &  169  &  3.2E-03  &  2.6E-03  &  18.1\% \\ \cline{2-6}
 &  7  &  225  &  2.6E-03  &  2.2E-03  &  12.2\% \\ \cline{2-6}
 &  8  &  256  &  2.4E-03  &  2.1E-03  &  11.6\% \\ \hline
\end{tabular}
\end{table}


\subsection{Global 1D-SpMV}
\subsubsection{CSR}
\begin{table}[htb]
\caption{Overall SpMV on Global CSR 1D-Row Partitioning(on Skylake).}
\label{tab:overall-spmv-csr-gk-single}
\centering
\begin{tabular}[c]{| l | c | c | c | c | c |}
\hline
\multirow{2}{*}{Matrices} & \multirow{2}{*}{Nodes} & \multirow{2}{*}{Processes} & \multicolumn{2}{| c |}{Time(s)} & \multirow{2}{*}{Error\%} \\ \cline{4-5}
  &  &  & Actual & Predicted &  \\ \hline
\multirow{4}{*}{333SP}  &  4  &  144  &  2.6E-02  &  2.1E-02  &  19.2\% \\ \cline{2-6}
  &  5  &  169  &  2.0E-02  &  2.0E-02  &  2.2\% \\ \cline{2-6}
  &  7  &  225  &  1.9E-02  &  1.8E-02  &  5.6\% \\ \cline{2-6}
  &  8  &  256  &  2.4E-02  &  1.9E-02  &  21.2\% \\ \hline
\multirow{4}{*}{AS365}  &  4  &  144  &  2.7E-02  &  2.2E-02  &  19.2\% \\ \cline{2-6}
  &  5  &  169  &  2.1E-02  &  2.0E-02  &  2.7\% \\ \cline{2-6}
  &  7  &  225  &  2.0E-02  &  1.9E-02  &  4.8\% \\ \cline{2-6}
  &  8  &  256  &  2.4E-02  &  1.9E-02  &  21.3\% \\ \hline
\multirow{4}{*}{M6}   &  4  &  144  &  2.5E-02  &  2.0E-02  &  19.2\% \\ \cline{2-6}
  &  5  &  169  &  1.9E-02  &  1.8E-02  &  3.2\% \\ \cline{2-6}
  &  7  &  225  &  1.8E-02  &  1.7E-02  &  4.5\% \\ \cline{2-6}
  &  8  &  256  &  2.2E-02  &  1.8E-02  &  21.1\% \\ \hline
\multirow{4}{*}{NLR}  &  4  &  144  &  2.9E-02  &  2.4E-02  &  19.2\% \\ \cline{2-6}
  &  5  &  169  &  2.2E-02  &  2.2E-02  &  2.3\% \\ \cline{2-6}
  &  7  &  225  &  2.1E-02  &  2.1E-02  &  4.1\% \\ \cline{2-6}
  &  8  &  256  &  2.6E-02  &  2.1E-02  &  21.3\% \\ \hline
\multirow{4}{*}{hugetrace-00010}  &  4  &  144  &  8.2E-02  &  6.6E-02  &  20.0\% \\ \cline{2-6}
  &  5  &  169  &  6.2E-02  &  6.1E-02  &  0.7\% \\ \cline{2-6}
  &  7  &  225  &  6.4E-02  &  5.7E-02  &  10.2\% \\ \cline{2-6}
  &  8  &  256  &  7.3E-02  &  5.7E-02  &  22.7\% \\ \hline
\multirow{4}{*}{road\_central} &  4  &  144  &  9.6E-02  &  7.6E-02  &  20.2\% \\ \cline{2-6}
\  &  5  &  169  &  7.1E-02  &  7.1E-02  &  0.5\% \\ \cline{2-6}
  &  7  &  225  &  7.4E-02  &  6.6E-02  &  10.3\% \\ \cline{2-6}
  &  8  &  256  &  8.6E-02  &  6.6E-02  &  23.4\% \\ \hline
\multirow{4}{*}{road\_usa}   &  4  &  144  &  1.6E-01  &  1.3E-01  &  19.3\% \\ \cline{2-6}
  &  5  &  169  &  9.9E-02  &  1.2E-01  &  23.8\% \\ \cline{2-6}
  &  7  &  225  &  9.8E-02  &  1.1E-01  &  16.5\% \\ \cline{2-6}
  &  8  &  256  &  1.5E-01  &  1.1E-01  &  23.1\% \\ \hline
\end{tabular}
\end{table}

\subsubsection{COO}
\begin{table}[htb]
\caption{Overall SpMV on Global COO 1D-Row Partitioning(on Skylake).}
\label{tab:overall-spmv-coo-gk-single}
\centering
\begin{tabular}[c]{| l | c | c | c | c | c |}
\hline
\multirow{2}{*}{Matrices} & \multirow{2}{*}{Nodes} & \multirow{2}{*}{Processes} & \multicolumn{2}{| c |}{Time(s)} & \multirow{2}{*}{Error\%} \\ \cline{4-5}
  &  &  & Actual & Predicted &  \\ \hline
\multirow{4}{*}{333SP}  &  4  &  144  &  3.3E-02  &  2.1E-02  &  34.4\% \\ \cline{2-6}
  &  5  &  169  &  2.7E-02  &  2.0E-02  &  26.0\% \\ \cline{2-6}
  &  7  &  225  &  2.4E-02  &  1.9E-02  &  23.9\% \\ \cline{2-6}
  &  8  &  256  &  2.7E-02  &  1.9E-02  &  30.9\% \\ \hline
\multirow{4}{*}{AS365}  &  4  &  144  &  3.3E-02  &  2.2E-02  &  34.4\% \\ \cline{2-6}
  &  5  &  169  &  2.7E-02  &  2.0E-02  &  26.1\% \\ \cline{2-6}
  &  7  &  225  &  2.5E-02  &  1.9E-02  &  24.3\% \\ \cline{2-6}
  &  8  &  256  &  2.8E-02  &  1.9E-02  &  31.0\% \\ \hline
\multirow{4}{*}{M6}  &  4  &  144  &  3.1E-02  &  2.0E-02  &  35.2\% \\ \cline{2-6}
  &  5  &  169  &  2.5E-02  &  1.9E-02  &  26.1\% \\ \cline{2-6}
  &  7  &  225  &  2.3E-02  &  1.7E-02  &  23.9\% \\ \cline{2-6}
  &  8  &  256  &  2.6E-02  &  1.8E-02  &  30.9\% \\ \hline
\multirow{4}{*}{NLR}   &  4  &  144  &  3.6E-02  &  2.4E-02  &  34.5\% \\ \cline{2-6}
  &  5  &  169  &  3.0E-02  &  2.2E-02  &  26.1\% \\ \cline{2-6}
  &  7  &  225  &  2.7E-02  &  2.1E-02  &  24.1\% \\ \cline{2-6}
  &  8  &  256  &  3.0E-02  &  2.1E-02  &  31.1\% \\ \hline
\multirow{4}{*}{hugetrace-00010}  &  4  &  144  &  9.3E-02  &  6.6E-02  &  28.6\% \\ \cline{2-6}
  &  5  &  169  &  6.7E-02  &  6.2E-02  &  7.6\% \\ \cline{2-6}
  &  7  &  225  &  7.0E-02  &  5.7E-02  &  18.3\% \\ \cline{2-6}
  &  8  &  256  &  7.9E-02  &  5.7E-02  &  28.2\% \\ \hline
\multirow{4}{*}{road\_central}  &  4  &  144  &  1.1E-01  &  7.7E-02  &  27.1\% \\ \cline{2-6}
  &  5  &  169  &  7.1E-02  &  7.2E-02  &  1.7\% \\ \cline{2-6}
  &  7  &  225  &  8.0E-02  &  6.7E-02  &  16.6\% \\ \cline{2-6}
  &  8  &  256  &  9.1E-02  &  6.6E-02  &  27.9\% \\ \hline
\multirow{4}{*}{road\_usa}  &  4  &  144  &  1.8E-01  &  1.3E-01  &  26.1\% \\ \cline{2-6}
  &  5  &  169  &  1.5E-01  &  1.2E-01  &  16.0\% \\ \cline{2-6}
  &  7  &  225  &  1.4E-01  &  1.1E-01  &  20.1\% \\ \cline{2-6}
  &  8  &  256  &  1.5E-01  &  1.1E-01  &  27.4\% \\ \hline
\end{tabular}
\end{table}


\bibliographystyle{unsrt}
\bibliography{spmv}
\end{document}