%\documentclass[conference, 10ppt]{IEEEtran}
\documentclass[sigconf,review,anonymous]{acmart}

 \AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}
\setcopyright{acmcopyright}
\copyrightyear{2020}
\acmYear{2020}
%\acmDOI{10.1145/1122445.1122456}
%
\acmConference[ICPP  '21]{ICPP '21: 50th International Conference on Parallel 
Processing (ICPP)}{August 9-12, 2021}{Chicago, Illinois, USA}
\acmBooktitle{Woodstock '21: 50th International Conference on Parallel Processing (ICPP),
  August 9-12, 2021, Chicago, Illinois, USA}
\acmPrice{15.00}


\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{comment}
%
%\usepackage{slashbox}
\usepackage{url}
%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm,algpseudocode}
\algrenewcommand\algorithmicindent{0.9em}%
\usepackage{soul}
\usepackage{xspace}
\usepackage{subcaption}
\usepackage[group-separator={,}, group-minimum-digits=4]{siunitx}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{xcolor,colortbl}


%
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
 
\begin{document}
%
\newcommand{\todo}[1]{\color{red}\textbf{\hl{#1}}\color{black}\xspace}
%\newcommand{\todo}[1]{}
\newcommand{\rom}[1]{\expandafter{\romannumeral #1\relax}}
%
\title{Performance Model of Iterated SpMV for Distributed System.}

\author{Md Maruf Hossain}
\affiliation{
  \institution{University of North Carolina at Charlotte}
  \city{Charlotte}
  \state{NC}
  \country{USA}
}
\email{mhossa10@uncc.edu}

\author{Erik Saule}
\affiliation{
  \institution{University of North Carolina at Charlotte}
  \city{Charlotte}
  \state{NC}
  \country{USA}
}
\email{esaule@uncc.edu}

\begin{abstract}
Many applications rely on basic sparse linear algebra operations from numerical solvers to graph analysis 
algorithms. Yet, the performance of these operations is still reasonably unknown. Users and practitioners 
rely on the rule of thumb understanding of what typically works best for some application domain.
\\ 
This paper aims at providing an overall framework for the distributed system to think about the performance 
of sparse applications. We use the sparse matrix-vector(SpMV) multiplication as the representative of the 
experiments. We model the performance of multiple SpMV implementations on distributed systems. 
 We model the performance of different modes of execution of SpMV using linear and polynomial regression models for a distributed system. 
 The models enable us to predict how to partition and represent the sparse matrix to optimize the performance of iterated SpMV on a cluster with 225 cores.
\end{abstract}
%
%%
%%

\keywords{SpMV, MPI, Graph Partitionings}

%
%%
%%
\maketitle
%%
\section{Introduction}
%%%%%%%%% Why %%%%%%%%%
Sparse matrix-vector multiplication(\textit{SpMV}) is one of the
fundamental operations in sparse linear algebra. It is critical to
solving linear systems and is widely used in engineering and
scientific applications~\cite{gleich2015pagerank, saad2003iterative,
  dytrych2016efficacy}.  Distributed memory systems have entered a new
age with the popularization of departmental clusters to support these
scientific and engineering applications.  Many different approaches
have been proposed to improve \textit{SpMV}. Just considering single
node efforts leads to stream of storage formats~\cite{ashari2014fast, ashari2014efficient, baskaran2009optimizing, bell2009implementing, buluc2011reduced, bulucc2009parallel, choi2010model, deng2009taming, garland2008sparse, greathouse2014efficient, kourtis2010exploiting, li2013smat, li2013gpu, liu2013efficient, su2012clspmv, tang2015optimizing, vuduc2005oski, williams2007optimization, yan2014yaspmv} and optimizations. 
%\todo{put a bunch of storage format papers}
\\
To perform sparse matrix-vector multiplication(\textit{SpMV}) on
distributed systems, the matrix is usually partitioned into multiple
parts and perform \textit{SpMV} on each part individually in the
different processors. Good partitioning can ensure better load balance
and limits the volume of communication between the underlying
\textit{MPI process}. 1D and 2D partitioning techniques are theMany partitioning algorithms have been proposed
to ensure good load balance and to minimize \textit{MPI communications}~\cite{deveci2015hypergraph, karypis1995multilevel,kaya2013analysis}.

A common problem is to choose which of these many approaches to use.
Often, picking the right approach to execute an application based on
SpMV relies on the rule of thumbs. Ideally when deploying an
application, the user should not have to pick a configuration, the
systems should be able to pick a good configuration automatically; or
at least provide a set of likely good candidate configuration. We
posit that building models to predict the performance of different
configurations is the only way to make better-informed
decisions. These models could also be used to better understand the
structure of the performance of SpMV based application across a wide
range of graph and computing platform.

In this paper, we study the modeling of performance of iterated SpMV
on distributed memory platform using MPI. We are not concerned with
achieving the highest performance, but rather modeling accurately the
performance of different configurations. We consider two classic
representation of sparse matrices, namely CSR and COO
representations. And we consider three distributed memory execution
model: 1D partitioning with global communication, and with custom
communications, and 2d partitioning with collective communications.

We adapt existing linear and polynomial \textit{Support Vector
  Regression}(\textit{SVR})~\cite{awad2015support} model first
proposed for GPUs to predict and analyze the run time of \textit{SpMV}
on distributed memory system. We also propose a model built from the
ground up from micro-benchmarks of the architectures.

We test the quality of the models of the execution of SpMV of seven
real world matrices and of randomly generated matrices on MPI clusters
up to 8 physical nodes (256 cores). The experiments show that the best
configuration of the execution of SpMV depend on the matrix. The
performance models usually have less than 10\% error in the predicted
runtime. That runtime prediction accuracy is enough to predict
accurately the configuration will lead to the best performance in most
cases.

We conclude by discussing the applicability of the different models in
practice.

\section{Sparse Matrix Vector Multiplication}

\subsection{Representation formats}

There are many representation format designed to store sparse matrices
for computing SpMV. While some representation format have been
designed especially to optimze SpMV. CSR and COO are generic storage
format that are used in most sparse linear algebra and graph
processing.

All the encoding of sparse matrices essentially aim at avoiding to
represent the zeros in matrices and only represent the non-zero values
in the matrix. We will assume the matrix $A$ is of size $n \times n$
and has $nnz$ non-zero values.

\subsubsection{Compressed Storage by Row (CSR)}

The Compressed Storage by Row format represents a matrix with three
arrays: \texttt{rowA}, \texttt{colA}, and
\texttt{valA}. Figure~\ref{fig:matrepcsr} shows an example matrix and
its CSR representation.

\begin{figure}
  \begin{subfigure}[b]{.3\linewidth}
    $\begin{pmatrix}
      1 & 2 & 3 & 0 & 0 \\
      0 & 0 & 4 & 5 & 6 \\
      0 & 2 & 0 & 0 & 0 \\
      1 & 0 & 0 & 0 & 3 \\
      0 & 0 & 3 & 4 & 0 \\
    \end{pmatrix}$

    \caption{A matrix}
  \end{subfigure}
  %
  \begin{subfigure}[b]{.6\linewidth}
    rowA = [ 0, 3, 6, 7, 9, 11 ] 
    
    colA = [ 0, 1, 2, 2, 3, 4, 1, 0, 4, 2, 3 ] 
    
    valA = [ 1, 2, 3, 4, 5, 6, 2, 1, 3, 3, 4 ]

    \caption{CSR Representation}
  \end{subfigure}
  
  \caption{Matrix representation format}
  \label{fig:matrepcsr}
\end{figure}

The \texttt{valA} array contains all the values that are non zero in
the matrix given in the order the appear in the matrix. That array is
of length $nnz$, the number of non zero in the matrix. The
\texttt{colA} also has $nnz$ values and it indicates for each non
zero, in which column of the matrix the non zero is located.

The \texttt{rowA} array is slightly more complicated, it has $n+1$
values. \texttt{rowA[r]} indicates the first index of the first
non-zero of row $r$ and the last value indicate the total number of
non zero in the matrix.

Computing $y = A x$ is done by computing each dimension of $y$
independently. For a particular row $r$, the algorithm extracts the
non-zeros with indices between $rowA[r]$ and $rowA[r+1]$. Each
non-zero $j$ is located in column $colA[j]$ and is of value
$valA[j]$. And the algorithm executes $y[r] += x[colA[j]]*valA[j]$.

\subsubsection{Coordinate (COO)}

The COO format is a simpler format. The non zeros do not have to be
listed in the order of the matrix, but they rather can be listed in
any order. As such, the non zeros of a row do not have to be listed
continously. To make this possible, the \texttt{rowA} array is not of
size $n+1$ but is of size $nnz$.



\subsection{Distributed Memory Execution}


\subsubsection{2D-Uniform partitioning}

In \textit{2D-Uniform partitioning},  matrices
are partitioned into both row-wise and column-wise.  There is a choice
in the number of column partitions and row partitions. However, for
iterated SpMV applications, it is common practice to pick a symmetric
partitioning scheme and a number of processors that is a perfect
square. Each MPI process handles one of the portions of the matrix.
Load imbalance is a common issue in 2D partitioning. Many matrices
tend to have a band structure that tends to build very imbalanced
partitioning.  2D-Uniform partitioning can balance the number of rows
and columns but can have a significant load imbalance in the number
of non-zeros in the blocks.

Boman and Devine et al.~\cite{boman2013scalable} noted that
randomizing the order of the matrix can provide good load balance for
SpMV in 2D-Uniform partitioning and is used in scientific
applications~\cite{dytrych2016efficacy}. We adopt this strategy for
our 2D uniform partitioning technique. Each row (and corresponding
vector entry) is assigned to a random process.  Since the expected
number of rows and non-zeros is uniform for all processes, this method
generally achieves a good load balance.

If $P=p^2$ is the number of processes
and $matrix\_size$ is the size of the matrix then each processor
should contain the same
number$\Bigl\lceil\dfrac{matrix\_size}{p}\Bigr\rceil$ of rows and
columns. The last block can be padded with zeros to make blocks of the name size.

With 2D-Uniform partitioning, the execution of the iterated SpMV is
done using the classic methods. Each process performs its local
multiplication. The values are then reduced within each row onto the
diagonal process. These reduced values are then broadcast along with the
column processes for the next iteration of the calculation.

\subsubsection{1D-Row Partitioning}

In \textit{1D-Row Partitioning}, matrices are split into row-wise
only. Balancing the load between the processors and minimizing the
communication boils down to solving a K-way graph partitioning
problem~\cite{kaya2013analysis}.  We can define the ID-Row(K-way)
partitioning for a graph $G=(V,E)$ with $|V|=n$, partition $V$ into
$k$ subsets, $V_1, V_2, \dots, V_k$ such that $V_i \cap V_j=\phi$ for
$i\neq j$, $|V_i| = n/k$, and $\cup_i V_i = V$, and the number of
edges of $E$ whose incident vertices belong to different subsets is
minimized. Each partition will allocate to a different MPI
process. We base the 1D row partitioning technique that we will model
on this strategy and we choose the METIS graph partitioning
tool~\cite{karypis1995multilevel} to provide the precise row
partitioning.

In 1D-Row partitioning, one process contains small portions of rows
and their corresponding columns. To perform SpMV on the 1D-Row
partitioning, a processor can hold either the portion of the \textbf{vector
  elements} corresponding to their row elements or the full vector.
We call the strategy where the process holds the entire vector \textit{Global
  1D-Row SpMV}(G1DR-SpMV). In this strategy, all MPI processes
perform matrix multiplication locally on the part of the matrix that
belongs to them. After matrix multiplication, an ALL$\_$Gatherv takes
place to share the updated value of the vector.

We call the strategy where a process only holds the portion of the
vector that it needs \textit{Local 1D-Row SpMV}(L1DR-SpMV).  In this
algorithm, initially, a process performs local matrix multiplication
on the non zero elements whose column belong to the local vector.  For
the rest of the non zero elements whose vector elements belong to the
other processes, it retrieves the relevant part of the vector using
communication tailored to each process using standard MPI point to
point communication primitives.

The METIS K-way partitioning technique minimizes the communications
performed by the Local 1D-Row SpMV. Therefore if METIS can partition
the graph well, one would expect the Local 1D-Row to perform very few
communications. However, if the partitioning is not good, the custom
message strategy only saves few communications and the Global 1D-Row
SpMV would benefit from the effectiveness of MPI Collectives. There is
no point in designing a local 2D scheme since the randomized
partitioning ensures there is little saving to be expected from
customized communications.

\subsection{Modeling performance}

There have been lots of previous work have done on the \textit{SpMV}
performance model for the \textit{CPU} and \textit{GPU}
architecture. In particular, Guo and Wang~\cite{guo2013performance}
\textit{et.al.} have proposed a linear model for the general-purpose
\textit{GPU} that can predict the runtime of the \textit{SpMV} based
on the strides size and nonzero per row. There are also some other
\textit{SpMV} models~\cite{nisa2018effective,guo2018performance} that
exist for the \textit{GPU} architectures. A performance model for
\textit{SpMV} on the \textit{GPU} architectures is more common than a
single or distributed system of \textit{CPU} architectures. Consistent
and parallelisms of the \textit{GPU} performance is the main reason
behind this.
model for the GPU. 


\section{Performance Model}
We build up three different SpMV models and investigate their performance on different algorithms and
storage formats.
\begin{enumerate}
\item SpMV Model from Micro-Benchmark.
\item Support Regression Model.
\item Linear Model.
\end{enumerate} 

\subsection{Linear Model}
\label{sec:linear-2d-spmv}
We build up a linear model from generated random matrices of the different
number of rows. For each matrix with specific rows, we generated
matrices with the different number of nonzeros per row $1, 2, 4, 8, 16,
\dots, 128$. Then build a linear regression model for a particular row
size matrix against the nonzeros per row. Our aim to find out two near
similar generated matrix $A$ and $B$ for given test matrix($M$), that
the number rows in $A$ and $B$ immediate lower and higher than $M$
respectively form the available matrices of the model. Then we predict
the performance of the test matrix $M$ based on these two model
matrices.

We realized in early investigations that the linear models would not
perform well on 1D partitioning models because the local number of non
zero varies significantly. However, in the uniform 2D-Partitioning
\textit{SpMV}, every process receives a matrix where non-zero are
distributed randomly.

Figure~\ref{fig:npr} shows the linearity of the run time over the
non-zero per row for a particular matrix. It confirmed that we should
build a regression model that can predict the run time of
\textit{SpMV} based on the non zero per row. We build multiple linear
models using the different sizes of the random matrix. Each model
trains by the different number of non-zeros per row but the same row
size.

If a test matrix has $r$ row and $npr$ non-zero per row, then our
system will find two linear models($L_1, L_2$) for row $r_1$ and $r_2$
like $r_1\leq r\leq r_2$.  It is important to note that, $L_1(r_1)$
and $L_2(r_2)$ are the two models with immediate smaller and larger rows
than $r$ respectively.  The following two equations represent the
linear model $L_1(r_1)$ and $L2(r_2)$,
\begin{equation*}
\begin{array}{l}
y_1\ =\ m_1\times x\ +\ c_1  \qquad\qquad\text{for } r_1 \ \text{and } x=npr\\
y_2\ =\ m_2\times x\ +\ c_2  \qquad\qquad\text{for } r_2 \ \text{and } x=npr
\end{array}
\end{equation*}

According to our model, we expect the performance($y$) of the subject
matrix with row $r$ is between $y_1$ and $y_2$ ($y_1\leq y\leq
y_2$). Our system finally predict the execution run time of the
subject matrix using the following equation,

\begin{equation*}
\begin{array}{l}
y\ =\ y_1+\frac{(y_2-y_1) (r-r_1)}{r_2-r_1}
\end{array}
\end{equation*}

\begin{figure}[hbt!]
  \centering
  
  \begin{subfigure}[b]{.48\linewidth}
    \includegraphics[width=\linewidth]{figures/model_nonzero_perRow_for_particular_row_process_100.pdf}
    \label{fig:npr}
    \caption{Nonzero per row vs run time for different matrices.}
  \end{subfigure}
  \begin{subfigure}[b]{.48\linewidth}
    \includegraphics[width=\linewidth]{figures/prediction_equation.pdf}
    \label{fig:linearspmvmodel}
  
    \caption{Sample equation for the linear model.}
  \end{subfigure}

  \caption{Linear relationship between nonzero per row and run time for the SpMV on the uniform 2d-Partitioning.}
  \label{fig:ov-linear-model}
\end{figure}

Figure~\ref{fig:linearspmvmodel} shows the example for the matrix with $250000$ average rows per process and 32 nonzero per row. 
Here, possible $r_1$ and $r_2$ equations are available for rows $200000$ and $310000$. The figure reflects the prediction mechanism of our 
system. 


\subsection{Polynomial Support Vector Regression(SVR) Model}
\label{sec:svr-spmv}
We build the polynomial SVR model for all three algorithms
(\textit{2D-Uniform partitioning SpMV, G1DR-SpMV, and L1DR-SpMV}). The
performance of the SVR models largely depends on the appropriate
attributes for the model. Then choose the proper values for the
\textit{free parameters} of the SVR model.  \textit{Cross-Validation}
and \textit{Grid-Search} widely use to do that for the SVR model. The
models are built for a particular system, therefore they assume a
fixed number of processors and interconnect.

\subsubsection{Cross-Validation and Grid-Search}
Initially, we split the matrices into train and test data set, no test
data participate in the training mechanism. Here, we choose the
polynomial (``poly'') kernel to perform all the machine learning
regression. In this kernel, some free variables need to choose. 
There are no fixed values for these variables, based on the
application criteria it can vary. We select $5-fold$ cross-validation
that means we split the training set into 5 different parts, and among
5 parts we choose 4 as a training set and the remaining one as a test
set. In the grid search, one need to select a set of variables for the
free variable($C,\gamma$), like $C=\{2^{-2},
2^{-1}, \dots, 2^4, 2^5\}\text{ and }\gamma=\{0.01, 0.02, \dots, 0.2, 0.3\}$
and for each pair of variable need to perform cross-validation and
record the score. Then select a pair that gives the best performance
in cross-validation.  The next step is to train the model using the best
variable on the whole train set.

\subsubsection{Feature selection}
We build up polynomial support vector regression(SVR) models for three
different SpMV algorithms on the two different graph formats(CSR,
COO).  The SVR model follows the same mechanism for all multiplication
algorithm. The model predicts the average run time for a particular
test matrix based on its attributes. So, attributes are the key to a
good performance model.  Although the local multiplication is the same
for every algorithm, the communications are different for the
different partitioning modes. The common attributes for all three
algorithms are:

\begin{enumerate}
\label{list:static-attributes}
\item Average rows per process.
\item Average non zero per process.
\item Average non-zero per row.
\item Density of the matrix.
\item Standard deviation of the non zero per row.
\end{enumerate}

But the communication among the MPI process in the Local L1DR-SpMV
(one-to-one) is different than the other two(one-to-all or all-to-all).
That is why based on the sparsity of the matrices the communication
can vary a lot. For that, we need the following extra attributes for
the L1DR-SpMV which can be extracted after partitioning.

\begin{enumerate}
\item Average local non-zero elements.
\item Average global non-zero elements.
\item Average inter-process call.
\item Average data transfer.
\end{enumerate}

\subsection{SpMV Model from Micro-Benchmark}
In this model, we predict the runtime of the SPMV based the performance of couple of micro-benchmark of 
a selected computer architecture. Initially, SpMV on the distributed system can be divided into two main parts,
\begin{itemize}
\item Core Calculation: performance of the matrix-vector calculation in a MPI node.
\item MPI Communication: communication runtime among MPI ranks(processes). 
\end{itemize}
\subsubsection{Instruction Cost}
We can represent the basic SpMV by $y=y+Val*x$, which requires two floating point operation(multiply and addition). 
So, we can say we need to calculate NNZ(number of non-zeros) times FMA(fused multiply addition) to perform SpMV.
But, first it needs to load the data and SpMV is bound by the memory bandwidth rather than instruction. So, we 
separated the \textit{core calculation} into,
\begin{itemize}
\item Run time for FMA: $L_{FMA}\times NNZ$, where $L_{FMA}$ is the latency of a single FMA and $NNZ$ is number of non-zeros.
\item Memory access latency: $L_{RW}$
\end{itemize} 



\paragraph{Micro Benchmark for \textit{FMA}}
The throughput of SkylakeX is 2 instructions per cycle and the latency of FMA is 4 cycles. So there is a potential 
of pipelinling($2\times 4 = 8$) to get the optimal results. Now, SkylakeX 
has 512-bit register that can give the ability of the vectorization. For 64-bits floating point operation it can give vector 
width 8 and for 32-bits it can give at max vector width 16. To find the peak performance of the FMA and to avoid the 
read-write latency we need to setup the benchmark that datasets can be contained in the register. Now, SkylakeX has 32 
registers. We can populate the pipeline by using sufficient ampunt of work. By varying the 
number of fused-multiply-addition calculation, we can find out the performance limitation. From the information of the 
processors, we can say that 4 cycles required for \textit{FMA} and the throughput of the FMA is 2 instruction per cycle, that means 
we should at least use $4\times 2=8$ instruction at a time to populate the pipeline.

\begin{figure}[hbt!]
  \centering
  \begin{subfigure}[b]{.48\linewidth}
    \includegraphics[width=\linewidth]{figures/fma/skylake_mpi_fma_roofline_model_for_vec_4.pdf}
    \label{fig:mpi-skl-vec-4}
    \caption{Vector Width: 4}
  \end{subfigure}
  %
  \begin{subfigure}[b]{.48\linewidth}
    \includegraphics[width=\linewidth]{figures/fma/skylake_mpi_fma_roofline_model_for_vec_8.pdf}
    \label{fig:mpi-skl-vec-8}
    \caption{Vector Width: 8}
  \end{subfigure}
  \caption{Skylake: (MPI)Roofline model for bandwidth for FMA operation}
  \label{fig:mpi-skl-fma}
\end{figure}

Figure~\ref{fig:mpi-skl-fma} shows the roofline model of the \textit{FMA} on SkylakeX processor. 
We can estimate the theoritical peak performance a single FMA by the following equation,
\begin{eqnarray*}
P\ =\ Base\_Clock\_Frequency\times Vector\_Width\times \frac{FLOPs}{Instruction}
\end{eqnarray*}
The significance of the \textit{FMA} 
latency is actually negligible compare to the memory access latency. 

\subsubsection{Memory Accesses Cost}

\paragraph{Micro-Benchmark for Memory Access}
We use the \textit{STREAM}~\cite{mccalpin1995stream} benchmark to find
out the cost the memory accesses for a selected architecture.

\begin{figure}[hbt!]
  \centering
  \begin{subfigure}[b]{.48\linewidth}
    \includegraphics[width=\linewidth]{figures/stream/skylake_mpi_copy_single_precision.pdf}
    \label{fig:stream-single-copy}
    \caption{Single Precision.}
  \end{subfigure}
  %
  \begin{subfigure}[b]{.48\linewidth}
    \includegraphics[width=\linewidth]{figures/stream/skylake_mpi_copy_double_precision.pdf}
    \label{fig:stream-double-copy}
    \caption{Double Precision.}
  \end{subfigure}
    
  \caption{Single and double precision memory access bandwidth on Skylake processor.}
  \label{fig:stream-copy-bandwidth}
\end{figure}

Figure~\ref{fig:stream-copy-bandwidth} shows the relation between data
size and bandwidth of the SkylakeX processor. To pick the right
bandwidth for a matrix-vector multiplication, we first calculate the
size of the data. Based on the size of the data, we pick the average
of the available immediate lower and higher point of the
benchmark. Note that in STREAM, all memory accesses are sequential.
 
\paragraph{Cache Access Patterns}
In SpMV some of the arrays are traverse sequentially and some have irregular
pattern. Table~\ref{tab:csr-spmv-2d-property} shows the memory access
property for different matrix representation format. We can see the
access to the vector \texttt{x} is irregular for both CSR and COO
format. But access to the vector \texttt{y} is only irregular for the
COO format.

One could model all irregular accesses as random accesses. However we
know that most graphs actually exhbit significant
locality~\cite{Beamer15}. We model the memory accesses by computing a
cache friendliness metric which represent the fraction of access that
are in cache and the fraction that are in memory.

This cache friendliness is computed for a matrix as follows. We assume
the matrix is traversed sequentially and we model the access to the
cache assuming the cache has the granularity of a cache line and that
the cache replacement policy is LRU. And we assume that the cache is
of the size of the L3 cache divided by the number of core on the
processor. This model does not accurately capture many properties of
the memory subsystem (such as associativity of caches, or cache
sharing across multiple cores, the fact that there are multiple levels
of caches, and concurrent processes); but it is a simple to compute
estimation of how irregular the memory access actually are.

All the sequential data accesses and \textit{hits} of irregular
accesses are treated as \textit{Sequential} data accesses. The cost of
these data accesses is accounted based on the predicted bandwidth to
the core out of the STREAM benchmark. And all cache \textit{miss} in
irregular accesses are treated to \texttt{Random} data accesses. The
cost of these operations are accounted based on the latency of the
memory access by the cores. The cost of these sequential and random
access are summed.

Note that this model does not capture all the complexity of modern
system: for instance, a mix of latency bound and bandwidth bound
memory operations can overlap especially when multiple cores access
memory at the same time. But while one could certainly craft an
example where the model is very inaccurate, we do not believe these
extreme case would happen in practice.

\begin{table}[htb]
\caption{Memory Access Property for 2D-Partitioning SpMV Model(RPP=rows per process, NNZ=non-zero elements).}
\label{tab:csr-spmv-2d-property}
\centering
\begin{tabular}[c]{| l | c | c | c | c | c |}
\hline
\multirow{2}{*}{Array} & \multicolumn{2}{ | c |}{\#Accesses} & \multirow{2}{*}{Data Type} & \multicolumn{2}{ | c |}{Access Type} \\ \cline{2-3} \cline{5-6}
  &  CSR & COO  & &  CSR & COO \\ \hline
rowA & 2$\times$RPP & NNZ & Integer & Sequential & Sequential \\ \hline
colA & NNZ & NNZ & Integer & Sequential & Sequential  \\ \hline
valA & NNZ & NNZ & Floating & Sequential &  Sequential \\ \hline
x & NNZ  & NNZ & Floating & Irregular &  Irregular \\ \hline
y & RPP & NNZ & Floating & Sequential & Irregular  \\ \hline
\end{tabular}
\end{table}



\subsubsection{Micro-Benchmark for MPI communication}
MPI communications mostly depend on the size of the message and
network topology. We build a benchmark based on the OSU-MPI-Benchmark.
Because of the dynamics of the network topology we build a polynomial
model based on the number of nodes, number of MPI ranks, message size,
and MPI communication type. \todo{What does that mean. Is that an SVR model?}

\begin{comment}
\begin{figure}[hbt!]
	\centering
	\subfigure[MPI All\_GatherV.]{\includegraphics[width=0.48\linewidth]{figures/osu/skylake_mpi_allgatherv.pdf}		\label{fig:osu-skylake-allgatherv}}
	\subfigure[MPI All\_Reduce.]{\includegraphics[width=0.48\linewidth]{figures/osu/skylake_mpi_allreduce.pdf}		\label{fig:osu-skylake-reduce}}
	\caption{Latency of the MPI collectives(All\_GatherV and All\_Reduce) on Skylake.}
	\label{fig:osu-collectives}
\end{figure}
\end{comment}

\subsubsection{Putting it together}

Figure~\ref{fig:spmv-model-from-benchmark} shows the overall structure of the SPMV model from micro-benchmark.

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=0.96\linewidth]{figures/spmv_hardware_model.pdf}
	\caption{Structure of the SpMV model from micro-benchmark.}
	\label{fig:spmv-model-from-benchmark}
\end{figure}





\section{Experimental Settings}
\subsection{Hardware Platform and Operating System}
All the nodes of the computing cluster come with Intel Xeon Gold
6154 processors (SkylakeX architecture) and 388GB of DDR4
memory. Each node has 36 cores in 2 sockets. Hyper-threading is
disabled. The base frequency of each processor is 3.00 GHz.  Each
processor has 25 MB of L3 Cache. The nodes are connected by EDR
Infiniband. The machine uses the \texttt{Linux}. To present
concise results all experiments are performed on 144, 169, 225 and 256 MPI processes
allocated on 4, 5, 7 and 8 nodes respectively. The system support maximum 256 MPI processes.

\subsection{Matrices}

All the matrices we use come from the SuiteSparse Matrix Collection
(previously known as the \textit{Florida Sparse Matrix}
collection)~\cite{Davis11}. We used seven matrices for our
tests. Their properties are described in Table~\ref{tab:test_mat_info}.
%\todo{make a table with stats of the matrices and reference it.}

Because the linear and SVR models require to be trained based on
timings from real runs, we used an other 83 matrices in order to train
these two models.

\begin{table}[htb]
\caption{Properties of the test matrices.}
\label{tab:test_mat_info}
\let\center\empty
\let\endcenter\relax
\centering
\resizebox{.8\width}{!}{\input{table/test_mat_info.tex}}
\end{table}

We also generated three larger matrices using the R-MAT
model~\cite{chakrabarti2004r}. R-MAT can generate graphs with
\textit{power-law} degree distributions and small-world
characteristics.  We generated the R-MAT matrices using parameter
$a=0.33$, $b=0.33$ and $c=0.33$. In the name of the matrix the first
numerical value in the name represent the total number of edges and
the second value represent the number of rows.

\subsection{Results}

We will resent two types of results, runtimes on particular matrices
and number of MPI processes for a particular execution mode of
SpMV. We will also present relative errors for models calculated by:
\begin{eqnarray*}
error =\ \frac{actual\ time - predicted\ time}{actual\ time}\times 100
\end{eqnarray*}

\todo{I suppose there should be a absolute value here?}

\section{Experimental Results}

\subsection{Runtime of SpMV}

Table~\ref{tab:actual_time_table} present the results of the different way of executing SpMV.

Looking first at the different 1D partitioning results. We can see
that having dedicated communication patterns to only access typically
delivers better performance than obtaining the complete.

The matrices in the testing set are mostly well partitionable. That
means that once partitione each node will not need many external
values of the x vector to perform the multiplication. Because of that,
the local variant of 1D partitioning which does custom messages is
particularly effective becasue it minimizes the total amount of value
exchange communications. The global variant uses MPI efficient
collective but communicates the entire vector to each node which is
more costly overall.

While local 1D partitioning perform better than the global exection
for all the matrices we picked and are listed
table~\ref{tab:actual_time_table}, it is because the global
communication of global 1D shares data that is not used. Indeed the
matrices of Table~\ref{tab:actual_time_table} are well
partitionable. But we expect that matrices that can not be well
partitioned would not see such poor performance when using the global
1D method. The performance of Global1D execution on R-MAT matrices,
which can not be well partitioned,
(Table~\ref{tab:rmat-actual_time_table}) is about the same as local
1D.

Similar reasons explains the performance of 2D partitioning methods.
The performance of SpMV on R-MAT is better on 2D uniform partitioning
than using 1D partitioning. Indeed the matrix is not well
partitionable, so METIS partitioning can not minimize the
communication of a 1D partitioning and suffers from load imbalance. On
the other hand, 2D partitioning will balance the load thanks to its
random row and column permutation. And the 2D decomposition of the
matrix provides more efficient communication patterns.

On the matrices of Table~\ref{tab:actual_time_table}, the random
permutation of rows and columns breaks all locality in the matrix and
cause communications that were not necessary in the 1D local
execution.

\subsection{Accuracy of performance models}

The linear model performs erratically on most of the testing data
set. And we only present the data for the 2D partitioned methods, the
model often exhibits an error higher than 15\%. The linear model can
not capture the behavior of the methods based on 1d partitioning and
is often predicts an order of magnitude away from the actual runtime
(error not shown).

The SVR model captures pretty well the performance of 2d partitioning
methods with errors of prediction below 15\% on most instances and
usually below 10\%. 2D partitioning with uniform permutation of rows
and columns provides the regularity in the dataset that SVRs can
easily capture.

Though the SVR model has moderate accuracy on 1D partitioning
model. This stems from the fact that the features of the SVR model do
not capture well the quality of the partitioning and the complexity of
the communication patterns.

The model based off micro benchmark overall performs the best. The
method using 1D partitioning with custom messages, Local 1D, is fairly
well models. All the instances are modeled within a 15\% of error and
often within 10\%. The model is accurate because the execution of SpMV
is carefully modeled and the communication pattern are also known to
the model.

The 2D partitioning execution are also well modeled with error usually
below 10\%. The error on the Global 1D execution is much higher. This
is due to the micro benchmark based model to not accurately predict
all gather collective. This MPI collective is fairly hard to model
accurately as the underlying communication algorithms can configure 
very differently way depending on the system state. The
2D partitioning also uses collective but in smaller communicators
which makes them easier to model.

The SVR model provides acceptable accuracy in its prediction of the
R-MAT SpMV performance.


\section{Discussion}

The micro-benchmarking based model and the SVR model are accurate
enough to predict the correct configuration of the system for a
particular matrix in most cases. When an error occurs, the loss of
performance for not picking the right model is usually under 10\%.

We believe there is more potential in the fine modeling of performance
of the micro-benchmark based model over the SVR model. The SVR model
is fairly expensive to train. It requires to build a different model
of the problem for each hardware configuration. The trained model is
different or 256 MPI processes than it is 225 processes. That means
each new hardware configuration needs to reexecute SpMV for all the
training matrices across all execution configurations. This is not a
scalable way to predict performance.

On the other hand, the micro-benchmark based model only requires to
run classical benchmark for the platform which are not specific to
SpMV. In other words, the cost of this model does not scale with the
number of hardware configuration or execution algorithms for SpMV.

Also, the SVR based model is hard to interpret. Once the model is
trained, even if it is accurate, the only thing we get is a third
degree polynomial. One can not easily pinpoint from the model why the
execution get the time it gets.

However, the finer model based on micro-benchmarks provides a direct
explanation of the runtime of the algorithm. One can easily understand
from the model how the runtime is formed. This can tell us for a
particular matrix whether the bottleneck is in the memory subsystem,
or in the MPI communication. This type of model is easier to explain.



\section{Conclusion}
In this paper, we provide three SpMV models to predict the run time of the
SpMV on the \textit{Uniform 2D-Partitioning}, \textit{GK-SpMV} and \textit{LK-SpMV}). 
The performance models can predict the run time accurately enough to identify the
optimal strategy to compute SpMV. Our models consider the matrix structure and partitioning information to 
predict the best possible communication strategy to perform
\textit{SpMV}. These models can be useful for the other graph
algorithms that heavily relies on the performance of the SpMV, such as
\textit{page rank}. The model may need to be adjusted to predict the
correct graph representation format.


\begin{table*}
\caption{Actual Run Time of All SpMV execution.}
\label{tab:actual_time_table}
\let\center\empty
\let\endcenter\relax
\centering
\resizebox{.88\width}{!}{\input{table/total_actual_time_info.tex}}
\end{table*}

\begin{table*}
\caption{Actual Run Time of All SpMV execution on R-MAT graph.}
\label{tab:rmat-actual_time_table}
\let\center\empty
\let\endcenter\relax
\centering
\resizebox{.88\width}{!}{\input{table/rmat_total_actual_time_info.tex}}
\end{table*}

\begin{table*}
\caption{All SpMV prediction model performance. \todo{Maybe geometric mean is the one we want?}}
\label{tab:performance_table}
\let\center\empty
\let\endcenter\relax
\centering
\resizebox{.58\width}{!}{\input{table/total_error_info.tex}}
\end{table*}

\begin{table*}
\caption{Performance of the SVR SpMV model on RMAT matrices(on SkylakeX).\todo{columns should be ordered like in the previous tables. Any chance we will get the models for linear and benchmark based?}}
\label{tab:performance_table_rmat}
\let\center\empty
\let\endcenter\relax
\centering
\resizebox{.99\width}{!}{\input{table/overall_svr_model_on_rmat_info.tex}}
\end{table*}


\bibliographystyle{unsrt}
\bibliography{spmv}
\end{document}






